{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with the OpenAI APIs"
      ],
      "metadata": {
        "id": "w09eNMVFbgXl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 1: Complete a simple request using the OpenAI APIs"
      ],
      "metadata": {
        "id": "_p25jglAbkQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intro\n",
        "\n",
        "In this lab you will learn how to access and use the OpenAI APIs to access models like GPT-3.5 or GPT-4 that are powering the ChatGPT service.\n",
        "\n",
        "Note: This lab requires you to sign up for the OpenAI platform. If you haven't done so yet, follow [these instructions here](https://www.educative.io/courses/open-ai-api-natural-language-processing-python/get-started-with-the-openai-api)\n",
        "\n",
        "**Please create a new API key for this lab so you can safely delete the key after this exercise.**\n",
        "\n",
        "**Learning goals:**\n",
        "* Understand how to store and use your Open AI API Key\n",
        "* Select a model from OpenAI\n",
        "* Send your first completion request."
      ],
      "metadata": {
        "id": "4_Nml2L4fHJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Set your API key"
      ],
      "metadata": {
        "id": "9r8sTMgJbrki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every interaction with one of the OpenAI APIs requires your unique OpenAI API key.\n",
        "\n",
        "Remember: Your API key is a secret! Anyone who has this key can use the OpenAI models on your behalf - so don't share it with others or expose it in any client-side code. Treat it like a password!\n",
        "\n",
        "A good practice is to never store your key inside the code for security resons. Instead, it's better to keep it as an environment variable.\n",
        "\n",
        "To define your API key as an environment variable using Python, run the following Python code and paste your API key when prompted\n",
        "\n",
        "(Visit your API Keys page to retrieve the API key you'll use in this requests - ideally, create a new API key which you can safely delete after this training)"
      ],
      "metadata": {
        "id": "rY8ZG8nocMKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option 1:** Get API key from local environment"
      ],
      "metadata": {
        "id": "fntEkpVIgved"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the API key locally - do this outside this notebook\n",
        "import os\n",
        "api_key = input(\"Enter your API key: \")\n",
        "os.environ[\"API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "enJHDL_Vd1LQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the API key from local environment variable\n",
        "import os\n",
        "api_key = os.environ.get('API_KEY')\n",
        "# Check if it's working\n",
        "print(api_key[:10]+'...')"
      ],
      "metadata": {
        "id": "NITfP4SQc1hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075bc356-d950-4f86-e2a4-67ceddab2076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sk-01OMqZP...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option 2:** Get API key from Colab key manager"
      ],
      "metadata": {
        "id": "TUjTIRhZg04h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('API_KEY')"
      ],
      "metadata": {
        "id": "ziWi3ypOg0SX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Choose a model"
      ],
      "metadata": {
        "id": "bJnSzurjbvkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have access to our OpenAI API key, we can start interacting with the OpenAI APIs.\n",
        "\n",
        "All API requests to OpenAI should include your API key in an Authorization HTTP header.\n",
        "\n",
        "Let's run the following `curl` request to retrieve the list of all available models:"
      ],
      "metadata": {
        "id": "dvNPcxd2es6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://api.openai.com/v1/models \\\n",
        "  -H \"Authorization: Bearer {api_key}\""
      ],
      "metadata": {
        "id": "u0kN8BGGb1UO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08516ea-5ee1-49e9-ba1b-786f78bc5799"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"object\": \"list\",\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"id\": \"text-search-babbage-doc-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"curie-search-query\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-davinci-003\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1669599635,\n",
            "      \"owned_by\": \"openai-internal\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-babbage-query-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649358449,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-search-query\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-babbage-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364043,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-similarity-davinci-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci-similarity\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"code-davinci-edit-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649880484,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"curie-similarity\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172510,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-search-document\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172510,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"curie-instruct-beta\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364042,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-ada-doc-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci-instruct-beta\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364042,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"whisper-1\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1677532384,\n",
            "      \"owned_by\": \"openai-internal\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-similarity-babbage-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-davinci-doc-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-similarity\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-embedding-ada-002\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1671217299,\n",
            "      \"owned_by\": \"openai-internal\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci-search-query\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-similarity-curie-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-davinci-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364042,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-davinci-query-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada-search-document\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada-code-search-code\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-002\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1692634615,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci-002\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1692634301,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci-search-document\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"curie-search-document\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172508,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-4-0613\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1686588896,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-code-search-code\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-ada-query-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"code-search-ada-text-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"babbage-code-search-text\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-4-vision-preview\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1698894917,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"code-search-babbage-code-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada-search-query\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1677610602,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada-code-search-text\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172510,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"tts-1-hd\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1699046015,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-curie-query-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-16k\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1683758102,\n",
            "      \"owned_by\": \"openai-internal\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-davinci-002\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649880484,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-davinci-edit-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649809179,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"code-search-babbage-text-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649357491,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-ada-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364042,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"ada-similarity\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"code-search-ada-code-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172507,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-similarity-ada-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172505,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-0301\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1677649963,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-4\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1687882411,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-16k-0613\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1685474247,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-search-curie-doc-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1651172509,\n",
            "      \"owned_by\": \"openai-dev\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"text-curie-001\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649364043,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"curie\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649359874,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"tts-1\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1681940951,\n",
            "      \"owned_by\": \"openai-internal\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-4-0314\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1687882410,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1649359874,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-1106\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1698959748,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1694122472,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-4-1106-preview\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1698957206,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"dall-e-2\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1698798177,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1692901427,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"gpt-3.5-turbo-0613\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1686587434,\n",
            "      \"owned_by\": \"openai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"tts-1-1106\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1699053241,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"tts-1-hd-1106\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1699053533,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"dall-e-3\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1698785189,\n",
            "      \"owned_by\": \"system\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci:ft-rapyd-ai-2023-02-03-07-29-05\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1675409345,\n",
            "      \"owned_by\": \"rapyd-ai\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": \"davinci:ft-rapyd-ai-2023-02-17-11-17-33\",\n",
            "      \"object\": \"model\",\n",
            "      \"created\": 1676632653,\n",
            "      \"owned_by\": \"rapyd-ai\"\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, OpenAI offers a large amount of models and it's easy to get overwhelmed. For most use cases, you will rely on the `gpt-3.5-turbo` model class, which OpenAI suggests as the default for most use cases."
      ],
      "metadata": {
        "id": "DT9sjUKNeIPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Using the OpenAI Python package"
      ],
      "metadata": {
        "id": "KP5HtIOwi0tQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A powerful alternative to querying the OpenAI APIs using `curl` is the Openai Python package. It gives us a lot of convenience functions that make working with the API easier.\n",
        "\n",
        "You can install the OpenAI python package using the following `pip` command:\n"
      ],
      "metadata": {
        "id": "m4gBdiCBe6Ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNoR9qApe_2d",
        "outputId": "a36308cd-8a18-4ebf-a5f8-5e07814347c6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.8-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's import the OpenAI Python package, define our API key and retrieve the model list using the following code:"
      ],
      "metadata": {
        "id": "yLwz-B5qjbCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = api_key)\n",
        "\n",
        "client.models.list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB97SKh3e7KH",
        "outputId": "3f27d203-dd4a-4237-994f-20e78e87f37f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SyncPage[Model](data=[Model(id='text-search-babbage-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-davinci-003', created=1669599635, object='model', owned_by='openai-internal'), Model(id='text-search-babbage-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='babbage', created=1649358449, object='model', owned_by='openai'), Model(id='babbage-search-query', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-babbage-001', created=1649364043, object='model', owned_by='openai'), Model(id='text-similarity-davinci-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='davinci-similarity', created=1651172509, object='model', owned_by='openai-dev'), Model(id='code-davinci-edit-001', created=1649880484, object='model', owned_by='openai'), Model(id='curie-similarity', created=1651172510, object='model', owned_by='openai-dev'), Model(id='babbage-search-document', created=1651172510, object='model', owned_by='openai-dev'), Model(id='curie-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-ada-doc-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='davinci-instruct-beta', created=1649364042, object='model', owned_by='openai'), Model(id='whisper-1', created=1677532384, object='model', owned_by='openai-internal'), Model(id='text-similarity-babbage-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-search-davinci-doc-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-similarity', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-embedding-ada-002', created=1671217299, object='model', owned_by='openai-internal'), Model(id='davinci-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='text-similarity-curie-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-davinci-001', created=1649364042, object='model', owned_by='openai'), Model(id='text-search-davinci-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='ada-search-document', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-code-search-code', created=1651172505, object='model', owned_by='openai-dev'), Model(id='babbage-002', created=1692634615, object='model', owned_by='system'), Model(id='davinci-002', created=1692634301, object='model', owned_by='system'), Model(id='davinci-search-document', created=1651172509, object='model', owned_by='openai-dev'), Model(id='curie-search-document', created=1651172508, object='model', owned_by='openai-dev'), Model(id='gpt-4-0613', created=1686588896, object='model', owned_by='openai'), Model(id='babbage-code-search-code', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-search-ada-query-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='code-search-ada-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='babbage-code-search-text', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-4-vision-preview', created=1698894917, object='model', owned_by='system'), Model(id='code-search-babbage-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada-search-query', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai'), Model(id='ada-code-search-text', created=1651172510, object='model', owned_by='openai-dev'), Model(id='tts-1-hd', created=1699046015, object='model', owned_by='system'), Model(id='text-search-curie-query-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-16k', created=1683758102, object='model', owned_by='openai-internal'), Model(id='text-davinci-002', created=1649880484, object='model', owned_by='openai'), Model(id='text-davinci-edit-001', created=1649809179, object='model', owned_by='openai'), Model(id='code-search-babbage-text-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='ada', created=1649357491, object='model', owned_by='openai'), Model(id='text-ada-001', created=1649364042, object='model', owned_by='openai'), Model(id='ada-similarity', created=1651172507, object='model', owned_by='openai-dev'), Model(id='code-search-ada-code-001', created=1651172507, object='model', owned_by='openai-dev'), Model(id='text-similarity-ada-001', created=1651172505, object='model', owned_by='openai-dev'), Model(id='gpt-3.5-turbo-0301', created=1677649963, object='model', owned_by='openai'), Model(id='gpt-4', created=1687882411, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-16k-0613', created=1685474247, object='model', owned_by='openai'), Model(id='text-search-curie-doc-001', created=1651172509, object='model', owned_by='openai-dev'), Model(id='text-curie-001', created=1649364043, object='model', owned_by='openai'), Model(id='curie', created=1649359874, object='model', owned_by='openai'), Model(id='tts-1', created=1681940951, object='model', owned_by='openai-internal'), Model(id='gpt-4-0314', created=1687882410, object='model', owned_by='openai'), Model(id='davinci', created=1649359874, object='model', owned_by='openai'), Model(id='gpt-3.5-turbo-1106', created=1698959748, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct-0914', created=1694122472, object='model', owned_by='system'), Model(id='gpt-4-1106-preview', created=1698957206, object='model', owned_by='system'), Model(id='dall-e-2', created=1698798177, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-instruct', created=1692901427, object='model', owned_by='system'), Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai'), Model(id='tts-1-1106', created=1699053241, object='model', owned_by='system'), Model(id='tts-1-hd-1106', created=1699053533, object='model', owned_by='system'), Model(id='dall-e-3', created=1698785189, object='model', owned_by='system'), Model(id='davinci:ft-rapyd-ai-2023-02-03-07-29-05', created=1675409345, object='model', owned_by='rapyd-ai'), Model(id='davinci:ft-rapyd-ai-2023-02-17-11-17-33', created=1676632653, object='model', owned_by='rapyd-ai')], object='list')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect a particular model - in this case the popular `gpt-3.5-turbo` model:"
      ],
      "metadata": {
        "id": "g927PVbHlQig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.retrieve(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "1yPcUCCzjy2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491c3bbe-6d38-4dcf-8c15-89bead388da8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(id='gpt-3.5-turbo', created=1677610602, object='model', owned_by='openai')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the response object you get. An interesting parameter here is the variable `created` which shows the date and time when the model was created in unix timestamp format.\n",
        "\n",
        "We can translate this into an actual date using the following Python code (replace the timestamp below with the one that you get from the response object):"
      ],
      "metadata": {
        "id": "j2VvdEuemsa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "timestamp = 1677610602\n",
        "date = datetime.utcfromtimestamp(timestamp)\n",
        "formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "formatted_date"
      ],
      "metadata": {
        "id": "wM_Lz1kcmNLW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c31f0f2a-9d2e-49c2-b9b3-25bd0c93559f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-02-28 18:56:42'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI is continuosly working on its models and releasing the most recent versions of it under the given model class, in this case `gpt-3.5.-turbo`. Why should you care? Well because that means if you're using the model class like this, you will always query the most recent version of the model. For some use cases, this makes sense, but for other use cases you want more control and keep the model you're using fixed - unless you're changing it explcitily.\n",
        "\n",
        "That's why OpenAI offeres seperate endpoints for variants of it's models. For example, if we want to use the GPT-3.5-Turbo model which was trained on June 13 (and keep this model fixed) we can query this model directly by using it's corresponding id:"
      ],
      "metadata": {
        "id": "CFeCPXnVnFTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.models.retrieve(\"gpt-3.5-turbo-0613\")"
      ],
      "metadata": {
        "id": "Y_Kd1IeSl-ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d767c88-a5be-4f79-889d-17a86c8e24a9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(id='gpt-3.5-turbo-0613', created=1686587434, object='model', owned_by='openai')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the timestamp if this model:"
      ],
      "metadata": {
        "id": "v6MPZLNimBHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp = 1686587434\n",
        "date = datetime.utcfromtimestamp(timestamp)\n",
        "formatted_date = date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "formatted_date"
      ],
      "metadata": {
        "id": "ES3xiPsgmj8D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e97be7bb-f0c7-4489-b4af-efd8ac160cf4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2023-06-12 16:30:34'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that this model was created on June 12, 2023 at 16:30:34. If we use the specific model id `gpt-3.5-turbo-0613` instead of the model class `gpt-3.5-turbo` we will always get this model."
      ],
      "metadata": {
        "id": "hboAG7XGoRPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Run your first completion request"
      ],
      "metadata": {
        "id": "aYsXjVbJbx9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to run our first completion request!\n",
        "\n",
        "The request below queries the most recent version of the `gpt-3.5-turbo` family to complete the text starting with a prompt of \"Berlin is the capital of\".\n",
        "\n",
        "Run the following Python code:\n"
      ],
      "metadata": {
        "id": "UYQfVNyNfN9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"The 3-Letter ISO code for France is\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ],
      "metadata": {
        "id": "48HdbZkfo9Xy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61aa1f29-32b2-41a9-d071-872b8a6c4c24"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletionMessage(content='FRA', role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should get a response back that completes this prompt with \"FRA.\" Note that exact completion you'll get might still vary, especially because we didn't adjust any of the model settings or response parameters.\n",
        "\n",
        "That's something we'll tackle in another lab!\n"
      ],
      "metadata": {
        "id": "Dn0QEgRmpSmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Congratulations!\n",
        "\n",
        "You have just completed your first request with the OpenAI APIs, setting you up for even more powerful use cases! You have learned how to securely store and retrieve your OpenAI API keys, how to choose a model (and keep it fixed), and how to use the OpenAI Python package to submit your first chat completion request."
      ],
      "metadata": {
        "id": "4R2jV_J0vhia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 2: OpenAI: Custom Text completion\n"
      ],
      "metadata": {
        "id": "kKAJ8o0vblzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this lab you will learn how to use OpenAI models such as GPT-3.5-Turbo or GPT-4 for simple text completion tasks using OpenAI's ChatCompletion API.\n",
        "\n",
        "Learning outcomes:\n",
        "* Lean how to use the ChatCompletion endpoint to generate text completions\n",
        "* Best practices around prompt design\n",
        "* Handling user inputs by adding variables to your prompt"
      ],
      "metadata": {
        "id": "8PybnS6QfWTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction\n"
      ],
      "metadata": {
        "id": "hkHMYu8ciVko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a completion scenario, a large language model will return one or more predicted completions, given a prompt. You can imagine this as some fancy sort of auto complete.\n",
        "\n",
        "\n",
        "For this exercise, we will rely on the `openai` Python package.\n",
        "\n",
        "First off, let's define our OpenAI API key as an environment variable:\n",
        "\n"
      ],
      "metadata": {
        "id": "Pg8MGc7IjWgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = api_key)"
      ],
      "metadata": {
        "id": "xZSihDmpsdpc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, let's load the `openai` package and create our first chat completion:"
      ],
      "metadata": {
        "id": "All-BJNqtE-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"The 3 letter ISO Code for France is:\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion)"
      ],
      "metadata": {
        "id": "PyCERu9itEee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06c41a3e-2043-4425-b419-4e69ab1736e6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-8UhVqQgLOWI9ypEB5Dl7HhPXCINHI', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='FRA', role='assistant', function_call=None, tool_calls=None))], created=1702327794, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=17, total_tokens=19))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print nicely\n",
        "import json\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fcAmmgMpMTe",
        "outputId": "b68709dc-ccfe-4cec-934a-a1c808a48382"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhVqQgLOWI9ypEB5Dl7HhPXCINHI',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'FRA',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327794,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 2, 'prompt_tokens': 17, 'total_tokens': 19}}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you've generated your first chat completion, let's break down the response object.\n",
        "\n",
        "We can see the `finish_reason` which indictates why the API stopped generating more tokens.\n",
        "\n",
        "When we see `stop` this means that the API returned the full completion generated by the model without running into any limits.\n",
        "\n",
        "We can also see the `usage` of our request. In this case, we used a total of 14 tokens - 12 for the prompt and 2 for the completion (the output). Bear in mind that the number of output tokens can be different if your output was something else than just \"Germany.\"\n",
        "\n",
        "**Controlling the completion output**\n",
        "\n",
        "The ChatCompletion function offers us various ways to tune the result of the model. You can find the full list of parameters [in the documentation](https://platform.openai.com/docs/api-reference/chat/create)\n",
        "\n",
        "Some notable parameters include:\n",
        "\n",
        "- `temperature`: This value between 0 and 2 controls the sampling temperature to use. Higher values will make the output more random, while lower values like will make it more focused and deterministic.\n",
        "- `n`: The number of chat completions to generate for each input prompt.\n",
        "- `max_tokens`:  The maximum number of tokens to generate in the chat completion. (The total length of input tokens and generated tokens is limited by the model's context length.)\n",
        "\n",
        "With that in mind, let's modify our previous request by making it a little more creative (higher `temperature`) and giving us some variations (more `n`) while not exceeding a certain amount of `max_tokens`:\n",
        "\n"
      ],
      "metadata": {
        "id": "0anulmL2gm5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"The 5 biggest cities of Germany are:\"}\n",
        "  ],\n",
        "  temperature=2,\n",
        "  n = 3,\n",
        "  max_tokens = 20\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "WyOssAJLvMmS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a8e0c20-750e-48ac-8bf0-cb4595ce49cc"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhWVZWICV8Dy8ICIKEbtCJTXfkxC',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'message': {'content': '1. Berlin - The capital city of Germany is the largest city in the country, with a population',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}},\n",
              "  {'finish_reason': 'stop',\n",
              "   'index': 1,\n",
              "   'message': {'content': '1) Berlin\\n2) Hamburg\\n3) Munich\\n4) Cologne\\n5) Frankfurt',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}},\n",
              "  {'finish_reason': 'length',\n",
              "   'index': 2,\n",
              "   'message': {'content': '1. Berlin- As the capital and and largest city of Germany, located in the northeastern part of',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327835,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 59, 'prompt_tokens': 16, 'total_tokens': 75}}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to run this code multiple times. You should observe that the model outputs varies and the finish reasons vary as well.\n",
        "\n",
        "When the `finish_reason` changed from `stop` to `length`, that's because the total number of `max_tokens` was hit."
      ],
      "metadata": {
        "id": "PX3RxXbNvvoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowing how completions work is good to understand how LLMs work conceptually.\n",
        "\n",
        "They try to predict the next token by token."
      ],
      "metadata": {
        "id": "mITy6eO7w5sQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Principle 1 - Be specific\n",
        "\n"
      ],
      "metadata": {
        "id": "aNfsLP75iews"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we get the desired outcome we want?\n",
        "\n",
        "A big part of that lies in a field called \"prompt engineering\" - this involves tuning your prompt in order to increase the probability to get an output that's useful for you.\n",
        "\n",
        "The most important piece is to make sure that requests provide any important details or context. Otherwise you are leaving it up to the model to guess what you mean.\n",
        "\n",
        "For Chat Completion models, a good place to provide more context is the `system` message.\n",
        "\n",
        "The system message can be used to specify the persona and behavior used by the model in its replies.\n",
        "\n",
        "For example, we could make our previous prompt more specific with the following `system` message."
      ],
      "metadata": {
        "id": "CQtAjHi4jzHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a geography expert. Complete the following sentence with the factually correct answer. Output one word only.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Berlin is the capital of...\"}\n",
        "  ],\n",
        "  temperature=2.0,\n",
        "  n = 3,\n",
        "  max_tokens = 10\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "eYoHJtTjjqGn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec191c8c-f03d-4efb-c378-165bcc16b3e1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhXV6Jte63PHx8iWmjBTGyWlpcQJ',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'Germany',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}},\n",
              "  {'finish_reason': 'stop',\n",
              "   'index': 1,\n",
              "   'message': {'content': 'Germany',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}},\n",
              "  {'finish_reason': 'stop',\n",
              "   'index': 2,\n",
              "   'message': {'content': 'Germany',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327897,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 3, 'prompt_tokens': 39, 'total_tokens': 42}}"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that even the temperature is as high as previously, it will (almost) always answer this question with the correct response \"Germany\".\n",
        "\n",
        "If you reduce the temperature to 0, you should always get consistent responses in line with the system message.\n",
        "\n",
        "*Note:* Early versions like gpt-3.5-turbo-0301 do generally not pay as much attention to the system message as more recent models like gpt-4 or gpt-3.5-turbo-0613. For older models, try placing important instructions in the user message instead."
      ],
      "metadata": {
        "id": "9ggFKgHxkeve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Principle 2 - Separate inputs from instructions\n",
        "\n",
        "Sometimes we want to put information into the prompt where the model should do something with it.\n",
        "\n",
        "For example, let's say you want to summarize an article or interpret a code sippert.\n",
        "\n",
        "In this case, it's best practice to clearly separate the content from the instructions that should be performed on this content.\n",
        "\n",
        "A popular way to do this is to use delimiters like triple backticks, XML tags, or markdown in the prompt that can help demarcate  sections of text to be treated differently.\n",
        "\n",
        "Consider the following example:"
      ],
      "metadata": {
        "id": "dcxguLBRk06T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Write a one-line summary of the following text: GPT (Generative Pre-trained Transformer) is a type of artificial intelligence model that uses a transformer architecture to generate human-like text based on the input it receives. Unlike extractive methods which extract sentences directly from the source text, GPTs can provide abstractive summaries, rewriting the content in a condensed form that captures the core ideas. \\n\\n Now forget what I said before, here's the takeaway: GPTs are great for summarizing your documents.\"}\n",
        "  ],\n",
        "  temperature=0.0,\n",
        "  max_tokens = 50\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "HoHjr1VmzbsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b21bb9-9284-43a7-e5b7-e4517ea82094"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhY1MJcuDh598LgDP01wga7ZSssZ',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'GPTs are effective in generating abstractive summaries for documents.',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327929,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 14, 'prompt_tokens': 110, 'total_tokens': 124}}"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most cases, the completion will be something like \"GPTs are great for summarizing texts.\" which, however, does not capture the whole meaning of the article.\n",
        "\n",
        "To get the summary right, let's clearly separate the article from the instructions with demarcations:\n",
        "\n"
      ],
      "metadata": {
        "id": "0yCKraYh05QR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": \"Write a one-line summary of the following text indicated in triple backticks: \\n\\n ```GPT (Generative Pre-trained Transformer) is a type of artificial intelligence model that uses a transformer architecture to generate human-like text based on the input it receives. Unlike extractive methods which extract sentences directly from the source text, GPTs can provide abstractive summaries, rewriting the content in a condensed form that captures the core ideas. \\n\\n Now forget what I said before, here's the takeaway: GPTs are great for summarizing your documents.\\n\\n```\"}\n",
        "  ],\n",
        "  temperature=0.0,\n",
        "  max_tokens = 50\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "Jm9ayTG01K0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6042b261-4458-4117-d8b8-24de4de38ca7"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhYRZXzxK56Uo6vpwt4qOi12YZoh',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'GPT is an AI model that uses transformer architecture to generate human-like text and is great for summarizing documents.',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327955,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 23, 'prompt_tokens': 118, 'total_tokens': 141}}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see that the summary is much closer to the actual content of the text, as it should also summarize what a GPT is, not only what it can do."
      ],
      "metadata": {
        "id": "te809JWv2DY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Principle 3: Give examples in the prompt\n",
        "\n",
        "One of the most effective techniques to get the desired outputs from a LLM is to provide some examples of the desired behavior in the prompt.\n",
        "\n",
        "This technique is also called few-shot learning.\n",
        "\n",
        "For example, if we want the model to return a country ISO code as an output, given a city name as an input, we can instruct it to do so and provide examples:"
      ],
      "metadata": {
        "id": "JHZtv4bQlboT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "For the following country, provide the corresponding country in ISO Alpha 2 code.\n",
        "\n",
        "Examples:\n",
        "Country: Germany\n",
        "ISO code: DE\n",
        "\n",
        "Country: France\n",
        "ISO code: FR\n",
        "\n",
        "Country: Spain\n",
        "ISO code: ES\n",
        "\n",
        "Country: United States of America\n",
        "Output:\n",
        "\"\"\"\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You're a dictionary for country name ISO codes\"},\n",
        "      {\"role\": \"user\", \"content\": prompt}\n",
        "  ],\n",
        "  temperature=0.0,\n",
        "  max_tokens = 1\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "l8JYdjtz3NeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8a7465-a0aa-430e-eab2-b937c07e52d1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhYmQUNsWwPH970Jq9IczyGQRW0u',\n",
              " 'choices': [{'finish_reason': 'length',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'US',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702327976,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 1, 'prompt_tokens': 75, 'total_tokens': 76}}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the completion was as expected \"US\".\n",
        "\n",
        "We can grab just the completion by selecting the element of the response object:"
      ],
      "metadata": {
        "id": "-6p6XrDI33j1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "_dJDzPnB4N6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a09392-cf71-411b-b87e-b50515412744"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "US\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While these are some of the core principles to use when working with LLMs, you can find many more of these in [OpenAI's GPT best practices](https://platform.openai.com/docs/guides/gpt-best-practices/)"
      ],
      "metadata": {
        "id": "XFVctsiHldgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Work with custom text"
      ],
      "metadata": {
        "id": "vJXgnCjkigMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's pull it all together and write a short function that takes inputs from a user, calls the Completion endpoint and returns this to the user.\n",
        "\n",
        "First, let's define the function:"
      ],
      "metadata": {
        "id": "vj3fUadD9IWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_country_iso_code(country):\n",
        "  country = str(country)\n",
        "  prompt = f\"\"\"\n",
        "  For the following country, provide the corresponding country in ISO Alpha 2 code.\n",
        "\n",
        "  Examples:\n",
        "  Country: Germany\n",
        "  ISO code: DE\n",
        "\n",
        "  Country: France\n",
        "  ISO code: FR\n",
        "\n",
        "  Country: Spain\n",
        "  ISO code: ES\n",
        "\n",
        "  Country: {country}\n",
        "  Output:\n",
        "  \"\"\"\n",
        "\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You're a dictionary for country name ISO codes\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.0,\n",
        "    max_tokens = 1\n",
        "  )\n",
        "\n",
        "  output = completion.choices[0].message.content\n",
        "  return output"
      ],
      "metadata": {
        "id": "CUkAn9na9TUf"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test if the function works by providing a new city name:"
      ],
      "metadata": {
        "id": "Ex_Rv5kL96d5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_country_iso_code(\"Poland\")"
      ],
      "metadata": {
        "id": "BiT4XMa698x5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "655f19e4-bee8-4315-89c4-19b49d79a489"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correct output here should be \"PL\" for Poland.\n",
        "\n",
        "Now, let's fetch some user inputs from the command line:\n"
      ],
      "metadata": {
        "id": "_GkBAdP4-cZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_country_iso_code(input(\"Which country do you need the country code for?\"))"
      ],
      "metadata": {
        "id": "uNo09Egm-d5S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6af83ef5-4386-4396-d75a-e026f0bef797"
      },
      "execution_count": 97,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Which country do you need the country code for?Brazil\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BR'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congrats! You just built your first LLM-powered micro service."
      ],
      "metadata": {
        "id": "fCQkIfNJ4ApA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Congratulations\n",
        "\n",
        "Congratulations on completing this lab on using the text completion capabilities from OpenAI!\n",
        "\n",
        "You've learned how to\n",
        "- query the API and understand how the ChatCompletion function works\n",
        "- apply the essentials of good prompt design, including specificity, content demarcation, and few-shot learning.\n",
        "- fetch user inputs and feed them into your prompt."
      ],
      "metadata": {
        "id": "KBSeeeZ-4TNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case 3: OpenAI: Chat completion"
      ],
      "metadata": {
        "id": "0skYbgSwboIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you will learn how to handle chat conversations using the OpenAI ChatCompletion APIs.\n",
        "\n",
        "Learning goals:"
      ],
      "metadata": {
        "id": "kkXPFHT65DyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction\n"
      ],
      "metadata": {
        "id": "jtewAn1_pEl-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat models like `gpt-3.5-turbo` and `gpt-4` take a list of messages as input and return a model-generated message as output.\n",
        "\n",
        "A sample conversation could look like this:\n",
        "\n",
        "```\n",
        "messages=[\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "      {\"role\": \"user\", \"content\": \"The FIFA World Cup in 2014 was won by the German national football (soccer) team. \"},\n",
        "      {\"role\": \"user\", \"content\": \"Where did it happen?\"}\n",
        "    ]\n",
        "```\n",
        "\n",
        "If you have taken the Text completion lab, then this should look very familiar to you!\n",
        "\n",
        "Before we get hands-on, let's briefly set up our enviornment by safely storing your OpenAI API key as an environment variable and importing the `openai` Python package:"
      ],
      "metadata": {
        "id": "DfxtXSY5m7WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import json\n",
        "client = OpenAI(api_key = api_key)"
      ],
      "metadata": {
        "id": "SFZzZbz97VRd"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example API call with the example above could look as follows.\n",
        "\n",
        "Run the following code to find out where the FIFA 2014 world cup was played."
      ],
      "metadata": {
        "id": "TsHz4m-I7UhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"The FIFA World Cup in 2014 was won by the German national football (soccer) team. \"},\n",
        "        {\"role\": \"user\", \"content\": \"Where did it happen?\"}\n",
        "      ]\n",
        ")\n",
        "\n",
        "json.loads(completion.json())"
      ],
      "metadata": {
        "id": "x8xdID_lm7Hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23647ce4-cff0-41a9-b184-92eb0613b71f"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-8UhbHaoNlHP58YjDeNBHfcbEqgaPo',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'message': {'content': 'The FIFA World Cup in 2014 was held in Brazil.',\n",
              "    'role': 'assistant',\n",
              "    'function_call': None,\n",
              "    'tool_calls': None}}],\n",
              " 'created': 1702328131,\n",
              " 'model': 'gpt-3.5-turbo-0613',\n",
              " 'object': 'chat.completion',\n",
              " 'system_fingerprint': None,\n",
              " 'usage': {'completion_tokens': 13, 'prompt_tokens': 48, 'total_tokens': 61}}"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, since this is the same ChatCompletion interface as we used in the lab for text completions, all tips and tricks regarding prompting also apply here. For a full documentation of the `chat` endpoint, see the [OpenAI documentation](https://platform.openai.com/docs/api-reference/chat)"
      ],
      "metadata": {
        "id": "2Xrc5ChzoMuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Chat Messages"
      ],
      "metadata": {
        "id": "9VpEQDFmpNs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main input for the `ChatCompletion` function is the messages parameter.\n",
        "\n",
        "So let's dive into this a bit more!\n",
        "\n",
        "Typically, a conversation is formatted with a system message first, followed by alternating user and assistant messages and their repsective content.\n",
        "\n",
        "- The `system` message helps set the behavior of the model, e.g. \"You are a helpful assistant.\"\n",
        "- The `user` user messages provide instructions or comments for the assistant to respond to.\n",
        "- The `assistant` messages store previous assistant responses, but can also be written by you to give examples of desired behavior (few-shot learning outside the system prompt)\n",
        "\n",
        "Chat conversations can be as short as one message or many back and forth turns and the LLM will be able to guess the context of a given word within this conversation.\n",
        "\n",
        "For example, when we asked the model `Where did it happen?` it figured out that we referred to the world cup final.\n",
        "\n",
        "However, this is only possible if this context is provided in the current `messages` object since **the model has no memory of past requests.**\n",
        "\n",
        "To demonstrate this behavior, run the following code which separeates the same messages into two different API queries:"
      ],
      "metadata": {
        "id": "3zZHTSILoTtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First request\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"The FIFA World Cup in 2014 was won by the German national football (soccer) team. \"},\n",
        "      ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)\n",
        "\n",
        "# Second request\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Where did it happen?\"}\n",
        "      ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "-kH_ckEm_nfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673d94bc-2556-4411-a3b6-7ce4d476e493"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, that's correct! The German national football team won the FIFA World Cup in 2014. The tournament took place in Brazil from June 12 to July 13, 2014. In the final match held at the Maracanã Stadium in Rio de Janeiro, Germany defeated Argentina 1-0 after extra time, with a goal scored by Mario Götze. This victory marked Germany's fourth World Cup title.\n",
            "I'm sorry, could you please provide more context or specify what \"it\" refers to?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model is not able to get the context of our second request, as this information was not provided in the prompt.\n",
        "\n",
        "In conclucsion, we always need to include the **whole conversation history** when we want users to refer to prior messages.\n",
        "\n",
        "And if a conversation gets so long that it does not fit within the model’s token (context) limit, it will need to be shortened in some way (such as only including the last few messages or summarizing the previous conversation), or we need to use advanced techniques such as [embeddings](https://platform.openai.com/docs/guides/gpt-best-practices/tactic-use-embeddings-based-search-to-implement-efficient-knowledge-retrieval) to make more information accessible to the model.\n",
        "\n",
        "For now, let's not bother about these advanced techniques but instead remember that we typically need to pass the whole conversation history into our `messages` object.\n",
        "\n",
        "This is a fundamental paradigm of chat models."
      ],
      "metadata": {
        "id": "rhM0Ku3I_6Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Building a simple chatbot - starting a conversation"
      ],
      "metadata": {
        "id": "F570sR0WpQF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a simple chatbot application that should act as a fitness coach and provide a custom fitness plan after asking the user a series of questions.\n",
        "\n",
        "Let's start by defining our system prompt, which provides the persona and instructions:"
      ],
      "metadata": {
        "id": "mJYywA7eB0iH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You are a professional fitness coach. Your task is to help the user achieve his fitness goals. To do this, you will ask the user a series of questions, one by one:\n",
        "\n",
        "Questions:\n",
        "- How old are you?\n",
        "- How much do you currently weigh?\n",
        "- What is your fitness goal? (e.g. gain muscles, reduce weight, increase stamina)\n",
        "\n",
        "From the given answers, put together a possible fitness plan. Keep your answers short, output the fitness plan as a table in markdown format.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EovNiyA4DZ_7"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's define the first question we want to show to the user to kick-off the conversation:"
      ],
      "metadata": {
        "id": "BtIjJfiXEVe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_message = \"How old are you?\""
      ],
      "metadata": {
        "id": "aampamxEEUyG"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need the LLM yet to get a response for this.\n",
        "\n",
        "Let's tackle it step by step and collect the answer for this question from the user.\n",
        "\n",
        "Run the following code and input a sample age:"
      ],
      "metadata": {
        "id": "4bY1Lp_NFJTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = input(initial_message)"
      ],
      "metadata": {
        "id": "8h8c-8TAFIuO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936a2214-c11a-42a7-fa47-0970da09d72e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How old are you?36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To recap, we have now three objects:\n",
        "- the system message with the desired chatbot behavior\n",
        "- the initial message we showed to the user\n",
        "- the user message which was the response to the initial message\n",
        "\n",
        "If you like, you can validate the ouputs of all three variables by running the following code:"
      ],
      "metadata": {
        "id": "8rCBDlQyFd5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(system_message)\n",
        "print(initial_message)\n",
        "print(user_message)"
      ],
      "metadata": {
        "id": "KXx0xn_eF2gG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe34b4f-d364-4a35-8585-192544754923"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a professional fitness coach. Your task is to help the user achieve his fitness goals. To do this, you will ask the user a series of questions, one by one:\n",
            "\n",
            "Questions:\n",
            "- How old are you?\n",
            "- How much do you currently weigh?\n",
            "- What is your fitness goal? (e.g. gain muscles, reduce weight, increase stamina)\n",
            "\n",
            "From the given answers, put together a possible fitness plan. Keep your answers short, output the fitness plan as a table in markdown format.\n",
            "\n",
            "How old are you?\n",
            "36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plug this all together to submit our first ChatCompletion request. We now want to see the response of our model."
      ],
      "metadata": {
        "id": "juswM9ebGClO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"assistant\", \"content\": initial_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "      ]\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=messages,\n",
        "  temperature = 0.1\n",
        ")\n",
        "\n",
        "completion.choices[0].message"
      ],
      "metadata": {
        "id": "o9ijeM-bB_oQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ec3171-4850-4133-b5bb-9c5687cee869"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletionMessage(content='How much do you currently weigh?', role='assistant', function_call=None, tool_calls=None)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model will ideally ask a follow-up question such as \"How much do you currently weigh?\""
      ],
      "metadata": {
        "id": "OAC53DU8GU1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Building a simple chatbot - Adding messages to a conversation"
      ],
      "metadata": {
        "id": "kpSFYLIRpRnR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's find out how we can add more user messages to the conversation we just started.\n",
        "\n",
        "First, let's take the last response from the model, show it to the user and collect the user input by running the following code:"
      ],
      "metadata": {
        "id": "j87i9c2MGeM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_message = input(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "PtYkGrYFGv3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0860142-b0ba-4010-a405-c6a9e71efafe"
      },
      "execution_count": 107,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "How much do you currently weigh?80kg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's append this chat completion along with the new user message to our previous message object:"
      ],
      "metadata": {
        "id": "53d3ylmWHTPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(completion.choices[0].message)\n",
        "messages.append({'role': 'user', 'content': user_message})"
      ],
      "metadata": {
        "id": "hJ9ZzTSgHbBf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the messages object now:"
      ],
      "metadata": {
        "id": "wsxDnoqxIZMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages"
      ],
      "metadata": {
        "id": "ekq4xPswHm6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcff2ce5-cbf3-462a-81e0-45624227df79"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\\nYou are a professional fitness coach. Your task is to help the user achieve his fitness goals. To do this, you will ask the user a series of questions, one by one:\\n\\nQuestions:\\n- How old are you?\\n- How much do you currently weigh?\\n- What is your fitness goal? (e.g. gain muscles, reduce weight, increase stamina)\\n\\nFrom the given answers, put together a possible fitness plan. Keep your answers short, output the fitness plan as a table in markdown format.\\n'},\n",
              " {'role': 'assistant', 'content': 'How old are you?'},\n",
              " {'role': 'user', 'content': '36'},\n",
              " ChatCompletionMessage(content='How much do you currently weigh?', role='assistant', function_call=None, tool_calls=None),\n",
              " {'role': 'user', 'content': '80kg'}]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, it contains all information from the ongoing chat history.\n",
        "\n",
        "We can now loop this process further until the chatbot responds with the desired fitness plan:"
      ],
      "metadata": {
        "id": "0RPFA4jGIbol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"\"\"\n",
        "You are a professional fitness coach. Your task is to help the user achieve his fitness goals. To do this, you will ask the user a series of questions, one by one:\n",
        "\n",
        "Questions:\n",
        "- How old are you?\n",
        "- How much do you currently weigh?\n",
        "- What is your fitness goal? (e.g. gain muscles, reduce weight, increase stamina)\n",
        "\n",
        "From the given answers, put together a possible fitness plan. Keep your answers short, output the fitness plan as a table in markdown format.\n",
        "\"\"\"\n",
        "\n",
        "initial_message = \"How old are you?\"\n",
        "user_message = input(initial_message)\n",
        "\n",
        "messages=[\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"assistant\", \"content\": initial_message},\n",
        "        {\"role\": \"user\", \"content\": user_message}\n",
        "      ]\n",
        "\n",
        "while True:\n",
        "  completion = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    temperature = 0.1\n",
        "  )\n",
        "\n",
        "  user_message = input(completion.choices[0].message.content)\n",
        "  messages.append(completion.choices[0].message)\n",
        "  messages.append({'role': 'user', 'content': user_message})\n",
        "\n",
        "  if user_message.lower() == 'exit':\n",
        "      print(\"Exiting the chatbot.\")\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4y_63EdIVOL",
        "outputId": "36224427-b951-43a3-d659-d7136acea06e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How old are you?36\n",
            "How much do you currently weigh?80kg\n",
            "What is your fitness goal? (e.g. gain muscles, reduce weight, increase stamina)gain muscles\n",
            "Based on your age, weight, and fitness goal, here is a possible fitness plan for you:\n",
            "\n",
            "| Fitness Plan |\n",
            "|--------------|\n",
            "| **Goal:** Gain muscles |\n",
            "| **Age:** 36 |\n",
            "| **Weight:** 80kg |\n",
            "| **Plan:** |\n",
            "| 1. Start with a strength training program that includes compound exercises such as squats, deadlifts, bench press, and overhead press. Aim to train 3-4 times per week. |\n",
            "| 2. Gradually increase the weight and intensity of your workouts to challenge your muscles and promote muscle growth. |\n",
            "| 3. Incorporate a balanced diet that includes lean protein sources, complex carbohydrates, and healthy fats to support muscle growth and recovery. |\n",
            "| 4. Ensure you are getting enough rest and recovery between workouts to allow your muscles to repair and grow. Aim for 7-8 hours of quality sleep each night. |\n",
            "| 5. Consider working with a personal trainer or fitness coach to ensure proper form and technique during your workouts and to provide guidance and support throughout your fitness journey. |\n",
            "\n",
            "Remember to listen to your body and make adjustments to your fitness plan as needed. Stay consistent and dedicated to your workouts and nutrition, and you will be on your way to achieving your goal of gaining muscles. Good luck!exit\n",
            "Exiting the chatbot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations! You've just learned the very foundations to build your own chatbot."
      ],
      "metadata": {
        "id": "wP7BM4a9I6X5"
      }
    }
  ]
}
